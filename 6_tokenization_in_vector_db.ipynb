{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b072f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Install required packages\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63418",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Tokenization in Vector Databases\n",
    "Tokenization is the process of breaking text into smaller pieces called tokens. It‚Äôs a crucial preprocessing step before generating embeddings or feeding input to LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28515a8b",
   "metadata": {},
   "source": [
    "## üìå Why Tokenization Matters\n",
    "- Ensures input fits within token limits of models (e.g., OpenAI models have max token limits)\n",
    "- Impacts cost and performance (OpenAI charges per token)\n",
    "- Affects how text is chunked and embedded\n",
    "\n",
    "Token ‚â† Word: Words like `ChatGPT` may tokenize into multiple parts depending on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7111672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tokenizing text using OpenAI's tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# Use encoding for OpenAI's text models (like GPT-3.5/4)\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "sample_text = \"LangChain is a framework to build LLM applications.\"\n",
    "tokens = enc.encode(sample_text)\n",
    "\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102ad36",
   "metadata": {},
   "source": [
    "## üß† How It Connects to Vector DBs\n",
    "- Most embedding models also use tokenization internally\n",
    "- Helps you chunk long documents effectively\n",
    "- Knowing token count helps optimize prompt design and chunking logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c38a5b",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- Tokenization breaks text into machine-readable pieces\n",
    "- Important for embedding, chunking, and cost control\n",
    "- Tools like `tiktoken` let you estimate and manage tokens for OpenAI and other models"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
